{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-13T16:00:46.382294Z",
     "start_time": "2024-07-13T16:00:44.734651Z"
    }
   },
   "source": [
    "import json\n",
    "! pip install pypdf"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\thinayani\\pycharmprojects\\rag_langchain\\.venv\\lib\\site-packages (4.2.0)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T07:52:17.936978Z",
     "start_time": "2024-07-15T07:51:45.824580Z"
    }
   },
   "cell_type": "code",
   "source": "! pip install -q -U google-generativeai",
   "id": "2b57df5d4c554a3a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T07:55:24.790090Z",
     "start_time": "2024-07-15T07:55:23.459336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def split_pdf(pdfPath):\n",
    "    loader = PyPDFLoader(pdfPath)\n",
    "    data = loader.load()\n",
    "    textSplitter= RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    pages = textSplitter.split_documents(data)\n",
    "    \n",
    "    return pages"
   ],
   "id": "77ec75c74cfa4338",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T06:52:35.851209Z",
     "start_time": "2024-07-15T06:52:25.550159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "pages = split_pdf(\"Alice_in_Wonderland.pdf\")\n",
    "faiss_index = FAISS.from_documents(pages, embedding_function)\n",
    "query = \"Why does Alice go down the rabbit hole?\"\n",
    "\n",
    "docs = faiss_index.similarity_search(query, k=4)"
   ],
   "id": "7d31bfe58448bb11",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T06:52:41.281405Z",
     "start_time": "2024-07-15T06:52:41.273907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = f\"For the following question: {query} , based on the story:\\n {pages}\"\n",
    "print(prompt)"
   ],
   "id": "d0d97168a5e661d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following question what is employee churn? based on following information [Document(metadata={'source': 'testDoc.pdf', 'page': 0}, page_content='https://doi.org/10.1007/s10844-020-00614-9\\nAnovelschemeforemployeechurnproblemusing\\nmulti-attributedecisionmakingapproach\\nandmachinelearning\\nNishantJain1·AbhinavTomar1·PrasantaK.Jana1\\nReceived:28December2019/Revised:27July2020/Accepted:30July2020/\\n©SpringerScience+BusinessMedia,LLC,partofSpringerNature2020\\nAbstract\\nEmployee churn (ECn) is a crucial problem for any organization that adversely affects its\\noverall revenue and brand image. Many machine learning (ML) based systems have been\\ndeveloped to solve the ECn problem. However, they miss out on some essential issues\\nsuch as employee categorization, category-wise churn prediction, and retention policy for\\neffectively addressing the ECn problem. By considering all these issues, we propose, in\\nthis paper, a multi-attribute decision making (MADM) based scheme coupled with ML\\nalgorithms. The proposed scheme is referred as employee churn prediction and retention'), Document(metadata={'source': 'testDoc.pdf', 'page': 0}, page_content='(ECPR). We first design an accomplishment-based employee importance model (AEIM)\\nthat utilizes a two-stage MADM approach for grouping the employees in various categories.\\nPreliminarily, we formulate an improved version of the entropy weight method (IEWM) for\\nassigning relative weights to the employee accomplishments. Then, we utilize the technique\\nfor order preference by similarity to ideal solution (TOPSIS) for quantifying the importance\\nof the employees to perform their class-based categorization. The CatBoost algorithm is\\nthen applied for predicting class-wise employee churn. Finally, we propose a retention pol-\\nicy based on the prediction results and ranking of the features. The proposed ECPR scheme\\nis tested on a benchmark dataset of the human resource information system (HRIS), and\\nthe results are compared with other ML algorithms using various performance metrics. We\\nshow that the system using the CatBoost algorithm outperforms other ML algorithms.'), Document(metadata={'source': 'testDoc.pdf', 'page': 0}, page_content='Keywords Employee churn ·Employee importance model ·Retention policy ·\\nCatBoost algorithm ·MADM method ·TOPSIS\\n/envelopebackNishant Jain\\nnishantjain.iit@gmail.com\\nAbhinav Tomar\\nprofession.abhinav@gmail.com\\n1Department of Computer Science & Engineering, Indian Institute of Technology (ISM),\\nDhanbad 826004, IndiaJournalofIntelligentInformationSystems(2021)56:2 79–023\\nPublishedonline: 29 2020  September'), Document(metadata={'source': 'testDoc.pdf', 'page': 1}, page_content='1 Introduction\\nEmployees are valuable assets for any organization. However, when some employees leave\\nan organization, its productivity, project continuity, and growth strategies are severely\\naffected on which the image and turnover of the organization depend. This also makes other\\nemployees quitting the organization (Rashid and Jabar 2016 ). This, in turn, causes large\\nexpenditure on hiring new employees, which is a time-consuming and expensive task. The\\nliterature (Brown and Wilson 2007 ) refers to this issue as employee churn (ECn) problem,\\nwhich draws significant attention for an efficient solution to remain productive and com-\\npetitive in today’s world. In this context, designing an effective scheme for employee churn\\nprediction and retention policy is an important application of machine learning (ML). Over\\nthe past few years, ML algorithms have been booming, and they are successfully applied in'), Document(metadata={'source': 'testDoc.pdf', 'page': 1}, page_content='various domain such as recommender system (Tarnowska et al. 2020 ), personal life event\\nprediction (Khodabakhsh et al. 2018 ), stock price prediction (Xiao et al. 2019 ) ,a n ds oo n .\\nThe motivation for adopting ML algorithms for the ECn problem is threefold. Firstly, the\\norganization does not have sufficient resources to predict employee churn manually. Sec-\\nondly, the data for ECn prediction is now available in considerable quantity, which should\\nbe appropriately utilized to make a necessary decision (Ghasemaghaei and Calic 2019 ).\\nThirdly, the available dataset is not erratic, i.e., the dataset is being continuously updated.\\nIn past years, quite a few ML-based solutions have been proposed for ECn and its related\\nproblems. For example, Saradhi and Palshikar ( 2011 ) described a customer churn problem\\nand presented a case study for building and comparing predictive employee churn models.\\nTarnowska and Ras ( 2018 ) discussed the solution for customer attrition problem based on'), Document(metadata={'source': 'testDoc.pdf', 'page': 1}, page_content='actionable knowledge. Fan et al. ( 2012 ) proposed a scheme based on hybrid clustering anal-\\nysis for predicting trends in employee turnover for technology professionals. Punnoose and\\nAjit ( 2016 ) proposed a method for turnover prediction, which is based on eXtreme Gradient\\nBoosting (XGBoost). Similarly, Gao et al. ( 2019 ) presented a weighted quadratic random\\nforest algorithm for employee turnover prediction. However, all of these studies overlook\\nthe categorization of the employees into different classes such as those who contribute or\\ndo not contribute significantly to the turnover of the organizations. Moreover, they have\\nnot considered any retention policy according to different categories of the employees for\\neffectively addressing the ECn problem. These facts underpin the need for further research.\\nIt is well noted that the importance of the employees in an organization can be judged\\nbased on multiple criteria, and thus it is a very challenging task to categorize them accord-'), Document(metadata={'source': 'testDoc.pdf', 'page': 1}, page_content='ingly. In recent years, multi-attribute decision making (MADM) has shown great potential\\nto deal with such problems. The MADM methods have been successfully applied to various\\nreal-world problems such as healthcare system (Mendoza-G ˙omez et al. 2019 ), market-\\ning and business management, human resources selection task (Krylovas et al. 2017 ), and\\nother areas, (Zhou et al. 2018 ), (Tomar and Jana 2018 ). Among numerous MADM meth-\\nods proposed so far, the technique for order preference by similarity to ideal solution\\n(TOPSIS) (Hwang and Yoon 1981 ) has gained enormous popularity due to its multi-fold\\nadvantages. It is a simple and comprehensible concept and has good computational effi-\\nciency, and the ability to measure the relative performance for each alternative in a simple\\nmathematical form (Yeh 2002 ). TOPSIS evaluates the performance of alternatives through\\nthe similarity with the ideal solutions. Moreover, the one that is nearest to the positive-'), Document(metadata={'source': 'testDoc.pdf', 'page': 1}, page_content='ideal solution and farthest from the negative-ideal solution would be the best alternative.\\nIn TOPSIS, it is necessary to determine the relative weights of the features for which sev-\\neral methods have been proposed. For example, Chu et al. ( 1979 ) used the weighted least\\nsquare method to determine the weights for the fuzzy set. Hamed Fazlollahtabar ( 2010 )u s e d280 JournalofIntelligentInformationSystems(2021)56:2 79–023'), Document(metadata={'source': 'testDoc.pdf', 'page': 2}, page_content='the analytic hierarchy process (AHP) (Saaty 1987 ) to ranking the automobile seat comfort\\nbased on consumer preferences. Entropy weight method (EWM) (Wang and Lee 2009 )i s\\nfrequently used in the literature as it is prone to information entropy or attribute data hetero-\\ngeneity. Therefore, it is ideal for distinguishing alternatives. In contrast to other weighting\\nmethods (Fazlollahtabar 2010 ; Saaty 1987 ), the EWM also decreases the effect of false or\\nartificial attributes’ information, subjectively used by the users.\\nMotivated with the aforementioned evidence, we propose, in this paper, an MADM and\\nML-based scheme for the employee churn problem and retention policy. We refer this\\nscheme as employee churn prediction and retention (ECPR). The ECPR is based on TOP-\\nSIS that integrates EWM as a weight estimation method. At first, we formulate a novel\\naccomplishment-based employee importance model (AEIM) in order to group the employ-'), Document(metadata={'source': 'testDoc.pdf', 'page': 2}, page_content='ees into three categories based on the accomplishment parameters. The AEIM integrates an\\nimproved version of EWM with TOPSIS for quantifying the importance of employees. We\\nthen predict the class-wise employee churn using the CatBoost algorithm and compare the\\nresults with state-of-the-art ML algorithms. Finally, a class-wise employee retention strat-\\negy is provided using a permutation-based feature importance method based on prediction\\nresults.\\nTo the best of our knowledge, we are the first to design an MADM based scheme for the\\nECn problem that jointly addresses the following issues: 1) It identifies valuable employees\\nby categorizing them, 2) It predicts category-wise employee attrition (aka churn) based on\\nensemble methods, and 3) It highlights the causing factors of attrition so as to strategize\\nbetter retention plans.\\nThe rest of this paper is organized as follows. Section 2reviews the previous researches,'), Document(metadata={'source': 'testDoc.pdf', 'page': 2}, page_content='emphasizing the employee churn problem. The working process of the proposed ECPR\\nscheme is explained in Section 3. Section 4presents the research evaluation methodology\\nalong with performance metrics. The experimental results and findings are summarized in\\nSection 5. Finally, Section 6concludes this paper.\\n2 Relatedworks\\nThe existing literature on the ECn problem has gained significant attention from researchers.\\nSeveral pioneering research works have been carried out on ECn prediction in human\\nresource analysis. The volume of the ECn research based on the theoretical hypothesis has\\nincreased dramatically since the last century due to the dynamic and competitive market\\npolicy (Hom et al. 2017 ). Many researchers found the vital relationship between the satis-\\nfaction level of the employees and their continuation with the organizations (Harter et al.\\n2002 ). Employees with excellent job satisfaction levels present fewer rates of absenteeism,'), Document(metadata={'source': 'testDoc.pdf', 'page': 2}, page_content='have significant contributions to the organization, and are extremely amenable to continue\\nserving the organization (Morrow et al. 1999 ). Job satisfaction is influenced by personal\\ncharacteristics like perception, cognitive ability, demographic variables, expectations, sense\\nof achievement as well as environmental factors (Frederiksen 2017 ). These studies pri-\\nmarily focus on the correlation between job satisfaction and employee churn. However,\\nthese schemes provided the solution for the ECn problem on the ground of theoretical\\nassumption.\\nRecently, the machine learning algorithms aspire to extract beneficial information and\\nhidden knowledge from the past available dataset. Some of the endeavours to predict ECn\\ninvolved the use of machine learning and data mining techniques (Sikaroudi et al. 2015 ).\\nTo date, there has been rising attention to ECn prediction using ML in tourism, health care,JournalofIntelligentInformationSystems(2021)56:2 79–023 281'), Document(metadata={'source': 'testDoc.pdf', 'page': 3}, page_content='trade, and industrial systems worldwide. Here, we review only relevant research works that\\nuse ML algorithms to solve the ECn problem.\\nRen et al. ( 2011 ) described the ECn problem for the travel agencies and analyzed the\\nsolution in terms of the strategic model. Sexton et al. ( 2005 ) proposed employee turnover\\nprediction based on neural network. However, they still have the scope of improvement\\nconcerning the retention policy and the prediction accuracy of employee attrition. Chien and\\nChen ( 2008 ) proposed the system to improve the retention rate based on effective personnel\\nselection. For this, they use association rule along with a decision tree to set up needful rules\\nto select a human resource. However, they have not considered employee categorization and\\ncategory-wise retention strategy.\\nAlthough a very few research studies focus on retaining valuable employees (Budhwar\\net al. 2006 ), it is not beneficial for the long-term smooth functioning of the organizations.'), Document(metadata={'source': 'testDoc.pdf', 'page': 3}, page_content='The reason is that the cost of recruiting new employees is far more as compared to keeping\\nthe existing employees. Our proposed scheme is inspired to make a retention policy for each\\nclass of employees.\\nIt is clear from Table 1that most of the works have been carried out on the predic-\\ntion only, and a few works have been done on the retention. Besides, none of the existing\\nschemes has considered the employee categorization based on their importance, while it is\\nan essential component for efficiently solving the ECn problem. We consider all the afore-\\nmentioned issues together for designing a complete scheme for effectively addressing the\\nECn problem with due emphasis on employee categorization based on their importance.\\nTable 1 Comparative summary of relevant research works that use ML algorithms to address the ECn\\nproblem\\nLiterature Dataset Prediction Retention ML model(s) Application\\ncategorization model model area\\nGao et al. ( 2019 ) No Yes No Improved RF Generalized'), Document(metadata={'source': 'testDoc.pdf', 'page': 3}, page_content='Sexton et al. ( 2005 ) No Yes Yes NN and MGA Generalized\\nChien and Chen ( 2008 ) No No Yes DT and AR Specialized\\nFan et al. ( 2012 ) No Yes Yes ANN and SOM Specialized\\nK o ha n dG o h( 1995 ) No Yes No LR, ANN, and RF Generalized\\nPunnoose and Ajit ( 2016 ) No Yes No XGBoost Specialized\\nDolatabadi and Keynia ( 2017 ) No Yes No ANN and SVM Generalized\\nYigit and Shourabizadeh ( 2017 ) No Yes No DT, LR, SVM,\\nKNN, RF, and\\nNBCGeneralized\\nFrierson and Si ( 2018 ) No Yes No LR and KMSF Specialized\\nProposed Yes Yes Yes LR, SVM, DT,\\nXGBoost, RF,\\nand CatBoostGeneralized\\nAbbreviations:\\nDT→Decision Tree; RF→Random Forest; NN→Neural Network;\\nAR→Association Rule; LR→Logistic Regression; SOM→Self-Organizing Map;\\nCatBoost →Categorical Boosting; MGA→Modified Genetic Algorithm; SVM→Support Vector Machine;\\nANN→Artificial Neural Network; KNN →k-Nearest Neighbors; NBC→Naive Bayesian Classification;'), Document(metadata={'source': 'testDoc.pdf', 'page': 3}, page_content='KMSF →Kaplan Meier Survival Function; XGBoost →eXtreme Gradient Boosting;282 JournalofIntelligentInformationSystems(2021)56:2 79–023'), Document(metadata={'source': 'testDoc.pdf', 'page': 4}, page_content='Fig.1 The working flow map for the proposed ECPR scheme\\nNote that the employee categorization based on multiple factors (or accomplishments) is\\nsolved as a multi-attribute decision-making (MADM) problem.\\n3 ProposedECPRscheme\\nHere, we first present an outline of the proposed scheme. The overall process is shown in\\nFig.1. It comprises of two phases. In the first phase, we propose an accomplishment-based\\nemployee importance model (AEIM) for categorizing employees into three classes based on\\ntheir importance in terms of productivity. The categorization is done by applying improved\\nentropy weight method (IEWM) on accomplishment attribute values of each employee\\n(called decision matrix) followed by TOPSIS (See Fig. 2for details). In the second phase,\\nwe apply various machine learning algorithms on each employee category (generated in the\\nfirst phase) for employee churn prediction and retention policy (See Fig. 3for details). To'), Document(metadata={'source': 'testDoc.pdf', 'page': 4}, page_content='illustrate the concept, each of these phases is explained step-wise on the human resource\\ninformation system (HRIS) dataset, collected from the Kaggle Website ( 2017 ). The dataset\\nhas 14,999 instances and ten attributes (aka features). Two attributes are of float type, six\\nare of integer type, and two are of a categorical type. Details of the attributes are shown in\\nTable 2.\\n3.1 OverviewofEWMandTOPSIS\\nThis section presents an overview of EWM and TOPSIS. The working process of EWM is\\nbased on Shannon’s entropy as explained in Shannon ( 2001 ). The entropy is a quantified\\nway of uncertainty in terms of probability theory. This concept is wholly adapted to cal-\\nculate the relative intensities of contrast models to reflect the average intrinsic information\\nforwarded for decision making. Wang et al. ( 2009 ) extended Shannon’s concept of entropy\\nmeasurement as a method of weighting. Entropy weight is a parameter that describes how'), Document(metadata={'source': 'testDoc.pdf', 'page': 4}, page_content='different alternatives approach each other concerning a particular attribute. The data source\\nwith lower probability values is considered to carry more information than that with higher\\nprobability values. That is to say, the former data are more important than the latter data and\\ndeserve higher weight (Lu and Yuan 2018 ). Entropy weights are used as input to the other\\nMADM methods (such as TOPSIS) in order to rank the alternatives.\\nTOPSIS, first proposed by Hwang et al. ( 1981 ), is one of the classical ranking methods\\nthat can evaluate multiple alternatives with the same attributes. It is a simple and under-\\nstandable concept with excellent computational efficiency and ability to measure the relativeJournalofIntelligentInformationSystems(2021)56:2 79–023 283'), Document(metadata={'source': 'testDoc.pdf', 'page': 5}, page_content='Fig.2 The working flow graph of EWM and TOPSIS\\nFig.3 The conceptual flow graph of the proposed AEIM model284 JournalofIntelligentInformationSystems(2021)56:2 79–023'), Document(metadata={'source': 'testDoc.pdf', 'page': 6}, page_content='Table2 Data attributes with their data type and description\\nAttribute name Data type Description\\nSatisfaction level Float This is the employee satisfaction level, which ranges\\nfrom 0 to 1.\\nLast evaluation Float This is the employer’s measured performance, which\\nalso ranges from 0 to 1.\\nNumber project Integer How many number of projects is assigned to an employee?\\nAverage monthly hour Integer How many average hours worked by an employee in a month?\\nTime spent Integer This is the number of years an employee has spent in\\nthe organization.\\nWork accident Integer If an employee has had an accident at work or not.\\nLeft Integer If the employee has quit the organization or not.\\nPromotion last5year Integer Whether or not an employee has earned a promotion\\nover the last five years.\\nDepartment Categorical Department of employment employee.\\nSalary Categorical Employee salary levels such as low, medium and high.'), Document(metadata={'source': 'testDoc.pdf', 'page': 6}, page_content='performance for each alternative in a simple mathematical form (Yeh 2002 ). The basic prin-\\nciple of TOPSIS is that the most desirable alternative is nearest to the positive-ideal solution\\nand farthest from the negative-ideal solution. In other words, the positive-ideal solution is\\nthe best value solution for each alternative by maximizing the profit criteria and minimiz-\\ning the cost criteria. On the contrary, the negative ideal solution is the worst value solution\\nfor each alternative by the profit criteria and minimizing the cost criteria. The step-wise\\nworking details of EWM and TOPSIS are shown in Fig. 2.\\n3.2 Employeecategorization\\nAccording to the ‘Pareto principle’ (Sanders 1987 ), it can be stated that 80% of the prof-\\nits of an organization are generated by 20% of its employees, and thus these employees are\\nprecious for the organization. Therefore, there is a need to differentiate among employees'), Document(metadata={'source': 'testDoc.pdf', 'page': 6}, page_content='and identify the valuable ones. One way is to differentiate the employees based on differ-\\nent types of accomplishments that vary from organization to organization. For example, a\\nhealthcare organization may have patient satisfaction, medical work experience, etc., as the\\naccomplishments on which the doctors can be evaluated. Table 3shows various employ-\\nees with their accomplishment types for different types of organizations. Another problem\\nis that a large organization may have an enormous number of employees, and there may\\nbe a chance that an abundant number of employees want to quit the organization (known\\nas attrition). Such a large number is indeed a challenge for retention because not all such\\nTable3 Some examples for accomplishment types in different organizations\\nType of organization Employee type Type of accomplishment\\nHealthcare Doctor Patient satisfaction, Medical work experience\\nWebsite design Web designer Hit rate, Strategy, Usability, Content'), Document(metadata={'source': 'testDoc.pdf', 'page': 6}, page_content='School/College Teacher/Professor Student passing rate, Student feedback\\nRestaurant Waiter Customer feedback, Service ratingJournalofIntelligentInformationSystems(2021)56:2 79–023 285'), Document(metadata={'source': 'testDoc.pdf', 'page': 7}, page_content='employees are profitable for the organization. It is challenging to invest in retaining all of\\nthese employees. Additionally, holding on all the employees are not equally important for\\nthe organization. Therefore, class-wise retention is more effective and productive.\\nWith this motivation, we propose a novel AEIM model (depicted in Fig. 3), which aims at\\ncategorizing the employees according to their importance in terms of productivity. It utilizes\\ndecision-making methods by contemplating different accomplishments of the employee,\\nsuch as their satisfaction level, last evaluation, number of projects, average monthly working\\nhours, and time spent in the organization. First, relative weights of different accomplish-\\nments are determined using an improved entropy weight method (IEWM). Next, TOPSIS is\\napplied to categorize the employees into various classes, i.e., Enthusiastic, Behavioral, and\\nDistressed. By enthusiastic employees, we want to mean the employees who are the most'), Document(metadata={'source': 'testDoc.pdf', 'page': 7}, page_content='productive for the organization. Similarly, the behavioral and distressed employees are those\\nwho are average and less productive, respectively. We now discuss in details the process of\\nemployee categorization as follows.\\n3.2.1 RelativeweightsofaccomplishmentsusingIEWM\\nIt is worth noting that for a decision-maker, it is quite challenging to quantify the importance\\nof two employees if they have the same type of accomplishments, such as an equal number\\nof projects done or equal time spent in the organization. However, both the accomplish-\\nment may not have equal importance according to decision-maker perception. For example,\\nin order to judge the importance of an employee, the number of projects done by him/her\\nmay be given more importance than the time spent in the organization or vice versa. There-\\nfore, while evaluating the employees’ importance, each type of accomplishment should have\\ndifferent weights in order to reach the correct conclusion. In view of the above facts, we'), Document(metadata={'source': 'testDoc.pdf', 'page': 7}, page_content='apply EWM for assigning the relative weight to various employee accomplishments. The\\nstep-wise procedure of EWM, along with a case study, is as follows. However, this may be\\nnoted that we incorporate some change in step 5 of the original EWM with the justification\\ntherein. This modification improves the accuracy of the relative weights, and hence EWM\\nis referred to as IEWM.\\nStep 1: Construct a decision matrix : We first build decision matrix X(=[xij])forn\\nnumber of employees with mnumber of accomplishments. The structure of Xis as\\nfollows.\\nX=A1A2···Am\\nE1\\nE2\\n...\\nEn⎡\\n⎢⎢⎢⎣x11x12... x 1m\\nx21x22... x 2m\\n............\\nxn1xn2... x nm⎤\\n⎥⎥⎥⎦(1)\\nwhere Eidenotes the employee i,i=1,...,n;Ajrepresents the jthaccomplishment,\\nj=1,...,m,a n dxijis the value of accomplishment Ajfor employee Ei.A ne x a m p l e\\ndecision matrix based on the benchmark HRIS employee dataset (refer to Section 3) is\\nshown in Table 4.\\nStep 2: Normalize the decision matrix : The number of accomplishment may be different'), Document(metadata={'source': 'testDoc.pdf', 'page': 7}, page_content='from each other having different set of values and meanings, thereby causing inconsistent\\ncomparisons. Therefore, the decision matrix Xneeds to be normalized to standardize the\\ndata. Although any normalization method can be used in MADM approaches, existing286 JournalofIntelligentInformationSystems(2021)56:2 79–023'), Document(metadata={'source': 'testDoc.pdf', 'page': 8}, page_content='Table4 Decision matrix based on HRIS employee dataset\\nSatisfaction level Last evaluation Number project Average monthly hour Time spent\\nA1 A2 A3 A4 A5\\nE1 0.68 0.53 4 197 3\\nE2 0.31 0.88 7 272 4\\nE3 0.91 0.86 6 262 6\\nE4 0.45 0.51 2 160 3\\nE5 0.82 0.87 5 283 4\\nE6 0.52 0.47 3 150 2\\nstudies suggest that most preferable method is vector normalization as it can gener-\\nate most consistent results (Lu and Yuan 2018 ). Thus, we employ vector normalization\\nmethod and calculate the normalized decision matrix R(=[rij])in which normalized\\nvaluerijis calculated as follows.\\nIf the accomplishment of the employee is positive, i.e, it should be maximum for better\\nperformance, then normalization is done using the following equation.\\nrij=xij√n∑\\ni=1,xij̸=0(xij)2,fori=1,2,...,n ;j=1,2,...,m (2)\\nIf the accomplishment of the employee is negative, i.e, it should be minimum for better\\nperformance, then normalization is done using the following equation.\\nrij=1\\nxij√n∑\\ni=1,xij̸=01'), Document(metadata={'source': 'testDoc.pdf', 'page': 8}, page_content='(xij)2,fori=1,2,...,n ;j=1,2,...,m (3)\\nStep 3: Characteristic proportion calculation for an employee : As the values of an\\naccomplishment varies with respect to the employees, we use characteristic proportion\\nthat implies the probability of an accomplishment value for a particular employee in all\\nthe other employees. We consider pijto be the characteristic proportion of AjforEi,\\ndefined by ( 4).\\npij=rij∑n\\ni=1rij,fori=1,2,...,n ;j=1,2,...,m (4)\\nThe value of pijlies in the range [0,1].\\nStep 4: Entropy value estimation for each accomplishment : Using the characteristic\\nproportion values, we estimate entropy value for each accomplishment according to ( 5).\\nej=−1\\nln(m)n∑\\ni=1pij.l n(pij),forj=1,2,...m (5)\\nwhere ejdenotes the entropy measure of Ajfor all employees having the range as [0,1].\\nFor a particular accomplishment j, if the difference among its values pijis higher for\\ndifferent i, then its entropy value ejis small. Note that the attribute with smaller entropy'), Document(metadata={'source': 'testDoc.pdf', 'page': 8}, page_content='value reflects larger amount of information, i.e., the attribute is more important and\\ndeserves higher weight (Lu and Yuan 2018 ).JournalofIntelligentInformationSystems(2021)56:2 79–023 287'), Document(metadata={'source': 'testDoc.pdf', 'page': 9}, page_content='Step 5: Computing the entropy weight for each accomplishment :L e twe(j)be the entropy\\nweight of the Aj, we calculate we(j)according to ( 6).\\nwe(j)=1−ej∑m\\nj=1(1−ej),0≤we(j)≤1,m∑\\nj=1we(j)=1( 6 )\\nA c c o r d i n gt o( 6), when all entropy values ej→1(j=1,2,...m ) , a small differ-\\nence among the entropy values will bring about the change in the corresponding entropy\\nweight. For example, let {0.9, 0.8, 0.7 }and{0.99, 0.98, 0.97 }be the estimated entropy\\nvectors for {A1,A 2,A 3}and{A4,A 5,A 6}accomplishment, respectively. Here, the\\ndifferences among the entropy values of the two vectors are not the same, however, they\\nturn out to have the same entropy weight vector {0.167, 0.333, 0.5 }when using ( 6). This\\nis obviously improper manner to assign weights because different entropy value vec-\\ntors provide different amounts of information, so they should be given different entropy\\nweights.\\nTo overcome this downside, we replace the expression of entropy weight in ( 6)a s\\nfollows:\\nwe(j)={'), Document(metadata={'source': 'testDoc.pdf', 'page': 9}, page_content='α×we1(j)+β×we2(j) ifej<1\\n0i f ej=1,( j=1,2,...m ) (7)\\nwhere,\\nwe1(j)=1−ej\\nm∑\\nj=1,ej̸=1(1−ej),w e2(j)=1/ej\\nm∑\\nj=1,ej̸=0(1/ej)(8)\\nHere,αandβare constants (such that α+β=1) and 0 ≤we(j)≤1,∑m\\nj=1we(j)=\\n1. When the entropy value ej→1(j=1,2,...m ) , a small difference among the\\nentropy values will result in a significant change of we2(j). This means that we1(j)and\\nwe2(j)are complementary to each other. At the same time, αandβcontrol the propor-\\ntions of these two weights, i.e., when αis close to 0, we1(j)will have little contribution\\ntowe(j),a n dw h e n βis close to 0, we2(j)will have little contribution to we(j).I n\\nanother words, ( 7)a n d( 8) can give a reasonable entropy weight regardless of the extreme\\nentropy values.\\nThe results obtained after applying IEWM ( Steps 2-5) on the employee data (shown\\nin Table 4) are presented in Table 5. In this paper, αandβare taken equal as 0.5 to show\\nthe equal impact of both the weights ( we1(j)andwe2(j)).\\nTable5 Results of IEWM computations'), Document(metadata={'source': 'testDoc.pdf', 'page': 9}, page_content='Satisfaction level Last evaluation Number project Average monthly hour Time spent\\nA1 A2 A3 A4 A5\\nej 0.9666 0.9796 0.9579 0.9829 0.9687\\nwe1(j) 0.2312 0.1414 0.2917 0.1186 0.2171\\nwe2(j) 0.2009 0.1983 0.2027 0.1976 0.2005\\nwe(j) 0.2161 0.1698 0.2472 0.1581 0.2088288 JournalofIntelligentInformationSystems(2021)56:2 79–023'), Document(metadata={'source': 'testDoc.pdf', 'page': 10}, page_content='3.2.2 CategorizingemployeesusingTOPSIS\\nHere, we apply TOPSIS for categorizing employees based on the relative weights obtained\\nin the preceding section. The step-wise procedure along with illustrations are given as\\nfollows.\\nStep 1: Compute the weighted normalized decision matrix : We multiply normalized\\nmatrix R(=[rij])((2)a n d( 3)) by the entropy weights of accomplishments to obtain the\\nweighted normalized decision matrix Z(=[zij]). The weighted normalized value zijis\\ncalculated as:\\nzij=rij×we(j), fori=1,2,...,n ;j=1,2,...,m (9)\\nTable 6shows the calculated matrix Zusing ( 9) for the employee data shown in Table 4.\\nStep 2: Obtain the positive and negative ideal solution : We define the positive ideal\\nsolution as I+\\njand the negative ideal solution as I−\\nj.\\nI+\\nj=max{z1j,...z nj}andI−\\nj=min{z1j,...z nj},for positive accomplishment\\nI+\\nj=min{z1j,...z nj}andI−\\nj=max{z1j,...z nj},for negative accomplishment\\n(10)'), Document(metadata={'source': 'testDoc.pdf', 'page': 10}, page_content='For positive accomplishments, the larger the accomplishment value is, the better suit-\\nability of the corresponding organization. For example, an employee having higher\\nsatisfaction level or last evaluation should be more valuable for the organization. On the\\ncontrary, negative accomplishments have opposite meaning.\\nStep 3: Determine separation measures for each employee : Separation measures are\\ncalculated using widely accepted Euclidean distance method. According to existing lit-\\nerature (Tas ¸abat 2019 ), other distance measures, such as Statistical, Manhattan, linear,\\nspherical, Hamming, Chebyshev distance, etc., can also be applied here. The separation\\nof each employees from positive ideal solution I+\\njand negative ideal solution as I−\\njis\\ncalculated using ( 11) as follows.\\nS+\\ni=\\ued6a\\ued6b\\ued6b√n∑\\ni=1(zij−I+\\nj)2andS−\\ni=\\ued6a\\ued6b\\ued6b√n∑\\ni=1(zij−I−\\nj)2where j∈{1,...,m} (11)\\nTable6 Weighted normalized decision matrix\\nSatisfaction level Last evaluation Number project Average monthly hour Time spent'), Document(metadata={'source': 'testDoc.pdf', 'page': 10}, page_content='A1 A2 A3 A4 A5\\nE1 0.0924 0.0517 0.0839 0.0560 0.0660\\nE2 0.0421 0.0858 0.1468 0.0773 0.0880\\nE3 0.1236 0.0839 0.1258 0.0744 0.1321\\nE4 0.0611 0.0497 0.0419 0.0455 0.0660\\nE5 0.1114 0.0848 0.1048 0.0804 0.0880\\nE6 0.0706 0.0458 0.0629 0.0426 0.0440JournalofIntelligentInformationSystems(2021)56:2 79–023 289'), Document(metadata={'source': 'testDoc.pdf', 'page': 11}, page_content='Table7 Importance score\\n(I-score ) of employees using\\nTOPSISEmployee S+\\niS−\\niCCi(I-score)\\nE1 0.1051 0.0706 0.4016\\nE2 0.0927 0.1254 0.5751\\nE3 0.0219 0.1545 0.8759\\nE4 0.1476 0.0295 0.1665\\nE5 0.0620 0.1168 0.6532\\nE6 0.1436 0.0354 0.1978\\nStep 4: Calculation of relative closeness of each employee to the ideal solution :T h e\\ncloseness coefficient CCi(between 0 −1) indicates the relative closeness of ithemployee\\nwhich is determined as follows.\\nCCi=S−\\ni\\nS−\\ni+S+\\ni,where i∈{1,...,n} (12)\\nStep 5: Class-wise categorization of employees based on importance score : Here, we\\nconsider the closeness coefficient ( CCi) values as the importance score ( I-score )o ft h e\\nemployees. Table 7shows each employee’s I-score , calculated by using 9-12(Steps 1-4)\\nof TOPSIS.\\nNext, we categorize the employees based on their I-score .L e tmaxR andminR\\nbe the maximum I-score and minimum I-score , respectively. Let Dbe defined as\\n(maxR −minR)/Nclass ,w h e r e Nclass is number of total employee classes. Then, I-'), Document(metadata={'source': 'testDoc.pdf', 'page': 11}, page_content='score ranges are mathematically defined (shown in Table 8) based on which the employee\\nare categorized into (Nclass =3)classes (i.e., Enthusiastic ,Behavioral ,a n d Dis-\\ntressed ). Table 8shows the class-wise categorization of six employees based on their\\nI-score (obtained in Table 7). The output of AEIM is further used in prediction and\\nretention phases.\\n3.3 Predictionofchurningemployeesfollowedbyretentionpolicymanagement\\nThe main objective of this step is to identify the employees from each employee class that\\nhas the highest probability of quitting next followed by determining the factors causing\\nemployee attrition. The proposed ECPR scheme integrating with prediction and retention\\nphase is shown in Fig. 4. The working process for the same can be described by the\\nfollowing steps.\\nStep 1 : In this step, the original employee dataset is divided into enthusiastic, behavioral,\\nand distressed employee dataset using AEIM model (defined in Section 3.2).'), Document(metadata={'source': 'testDoc.pdf', 'page': 11}, page_content='Table8 Employee categorization into respective classes\\nEmployee class I-score range formula I-score range values Employee\\nEnthusiastic (minR +2∗D, maxR ] (0.6395 ,0.8759 ] E3,E5\\nBehavioral (minR +D, minR +2∗D] (0.4030 ,0.6395 ] E2\\nDistressed [minR, minR +D] (0.1665 ,0.4030 ] E1,E4,E6290 JournalofIntelligentInformationSystems(2021)56:2 79–023'), Document(metadata={'source': 'testDoc.pdf', 'page': 12}, page_content='Fig.4 The conceptual flow graph for predicting churning employees followed by retention policy manage-\\nment for each employee class\\nStep 2 : The primary goal of this step is to show the CatBoost is the accurate prediction\\nmodel for the ECn problem. In this paper, we compare the result of CatBoost algo-\\nrithm with three individual machine learning algorithm, namely support vector machine\\n(SVM) (Cortes and Vapnik 1995 ), logistic regression (LR) (Webb et al. 2011 ), decision\\ntree (DT) (Jin et al. 2009 ) and two tree-based ensemble machine learning algorithm,\\nnamely random forest (RF) (Breiman 2001 ), extreme gradient boosting (XGBoost)\\n(Chen et al. 2015 ).\\nStep 3 : Next, we use the test dataset to estimate how well it performs on unseen data\\nto determine the generalization error after evaluating the model best mounted on the\\ntraining dataset.\\nStep 4 : Here, we get the class-wise employees who may quit the organization. The'), Document(metadata={'source': 'testDoc.pdf', 'page': 12}, page_content='output of each class of the employee (i.e., Enthusiastic ,Behavioral ,a n d Distressed ) from\\nthe prediction phase is used for the retention phase.JournalofIntelligentInformationSystems(2021)56:2 79–023 291'), Document(metadata={'source': 'testDoc.pdf', 'page': 13}, page_content='Step 5 : In this step, the primary goal is to find the top reasons for employee quitting. For\\nthis, first, we rank the important features causing employee attrition using permutation-\\nbased feature importance method. Feature values are shuffled randomly in this method,\\none column at a time, and the model’s performance is measured before and afterwards.\\nThe results returned by the method reflect the changeover in a trained model’s perfor-\\nmance after permutation. Essential features are generally more susceptible to the method\\nof shuffling, resulting in more exceptional results of significance. Fisher et al. (Fisher\\net al. 2018 ) presented the algorithm for permutation-based feature importance method.\\nThe pseudo-code for this algorithm is expressed as Algorithm 1.\\nThe rationale behind using permutation-based feature importance method with the\\ntree-based strategies is that they rank naturally by how well they enhance node purity.'), Document(metadata={'source': 'testDoc.pdf', 'page': 13}, page_content='In addition to this, nodes with the most significant reduction in contamination occur\\nat the beginning of the trees, whereas nodes with the least reduction in contamination\\noccur at the end of the trees. Thus, we can create a subset of the essential features by\\npruning trees below a particular node. Next, we select the top-k important features and\\ntheir importance value from each employee class.\\nStep 6 : Finally, using the top reasons for an employee quitting along with the result\\nof step 4, we make the class-wise retention strategy. The goal is to highlight the prime\\nreasons contributing to employee attrition so that an organization can make an effec-\\ntive decision support retention policy. Ultimately, we give the weights to the respective\\nfeatures according to their importance value so as to prioritize them while making a\\nretention strategy. What needs to be pointed out is that handling the identified fea-'), Document(metadata={'source': 'testDoc.pdf', 'page': 13}, page_content='tures depends on the nature and requirements of an organization. However, our retention\\nstrategy can be applied in the real-time environment of any organization.\\n4 Evaluationmethodology\\nIn this section, we discuss and present the methodology for evaluating the proposed scheme.\\nThe ECPR scheme is implemented in Python version 3.6 with the NumPy, SciPy, SciKit-\\nLearn, Pandas, and Matplotlib libraries. We perform all the experiments on the Intel Core-\\ni7-6500U 2.5 GHz processor and 8 GB of RAM on a 64-bit platform.292 JournalofIntelligentInformationSystems(2021)56:2 79–023'), Document(metadata={'source': 'testDoc.pdf', 'page': 14}, page_content='4.1 Baselinealgorithms\\nHuman resource information systems typically have categorical features. This makes the\\nCatBoost algorithm better suited to the ECn prediction task. The prediction results are\\ncompared with that of three individual machine learning algorithm, namely support vec-\\ntor machine (SVM) (Cortes and Vapnik 1995 ), logistic regression (LR) (Webb et al. 2011 ),\\ndecision tree (DT) (Jin et al. 2009 ) and two tree-based ensemble machine learning algo-\\nrithm, namely random forest (RF) (Breiman 2001 ), extreme gradient boosting (XGBoost)\\n(Chen et al. 2015 ). It is well known that CatBoost is a modified gradient boosting decision\\ntree algorithm (GBDT) which uses symmetric trees. It helps to decrease prediction time and\\nerror in the ECn problem, due to the inclusion of a large number of ‘categorical features’\\n(Prokhorenkova et al. 2018 ). Moreover, the following factors make CatBoost the best choice\\nto use for our proposed scheme.'), Document(metadata={'source': 'testDoc.pdf', 'page': 14}, page_content='•Handling categorical features: One common technique for dealing with categorical fea-\\ntures is one-hot encoding (Micci-Barreca 2001 ). However, this encoding of individual\\nfeatures with high cardinality contributes to algorithm inefficiency. Since algorithms\\noffer more importance to continuous features than to dummy features, which mask\\nthe order of significance of the feature resulting in more unsatisfactory results (Par-\\ngent et al. 2019 ). In the CatBoost, these issues raised by one-hot encoding can be\\nresolved by grouping the categories into a limited number of clusters before applying\\none-hot encoding. A popular way is to group the categories by using target statistics\\n(TS) (Micci-Barreca 2001 ). Prokhorenkova et al. ( 2018 ) proposed a more effective TS\\nstrategy in the CatBoost algorithm, namely ordered TS to group the categories.\\n•Feature blend : The CatBoost produces feature combination by constructing a base tree'), Document(metadata={'source': 'testDoc.pdf', 'page': 14}, page_content='with a root node composed of only one feature, and randomly selects the other best\\nfeature for the child nodes and represents it along with the root node feature. This\\nfeature of the CatBoost helps to reduce the dimensionality of the ECn problem.\\n•Faster churn prediction : The CatBoost algorithm utilizes oblivious trees as base predic-\\ntors where the same dividing criterion is used throughout the entire tree level (Kohavi\\nand Li 1995 ). These trees are balanced, so they are less likely to over-fit. Each leaf\\nindex is encoded as a binary vector in oblivious trees, with a length equal to the size of\\nthe tree. This principle is commonly used for estimating model predictions in CatBoost\\nmodel evaluators, since all binaries use float, statistics and one-hot encoded functions.\\nAs a consequence, the employee’s churn prediction produces better result.\\n•Unbiased boosting : The traditional GBDT techniques pose a statistical issue, namely,'), Document(metadata={'source': 'testDoc.pdf', 'page': 14}, page_content='prediction shift during execution. In order to conquer the prediction shift problem, the\\nCatBoost adopts ordered boosting in which different permutations are used for training\\ndifferent models. As a result of multiple permutations, the CatBoost demonstrates lower\\nbias than the traditional GBDT techniques (Prokhorenkova et al. 2018 ). Therefore, the\\nprediction of the employee churn is expected to be more generalized.\\n4.2 Performancemetrics\\nThe objective of the ECPR scheme is to maximize the positive performance of the prediction\\nmodel on a test dataset for which the actual values are known. The ‘Confusion Matrix’ is\\nused to evaluate the experimental results, which is a standard evaluation criterion in the\\narea of machine learning. This provides a summary of the prediction results for a specific\\nproblem. In the case of the confusion matrix, there exist four following cases.JournalofIntelligentInformationSystems(2021)56:2 79–023 293'), Document(metadata={'source': 'testDoc.pdf', 'page': 15}, page_content='–L e t Tpbe the ‘True positive’ in which the attribute labeled as ‘left’ and is predicted to\\nbe ‘left’.\\n–L e t Tnbe the ‘True negative’ in which the attribute labeled as ‘left’ and is not predicted\\nto be ‘left’.\\n–L e t Fpbe the ‘False positive’ in which the attribute not labeled as ‘left’ and is predicted\\nto be ‘left’.\\n–L e t Fnbe the ‘False negative’ in which the attribute not labeled as ‘left’ and is not\\npredicted to be ‘left’.\\nThe detailed explanation of the metrics used for performance analysis is given as follows.\\nClassification Rate/Accuracy (ACC) : It is defined as the total number of correctly pre-\\ndicted churners which is calculated as follows:\\nACC =Tp+Tn\\nTp+Tn+Fp+Fn(13)\\nRecall (RC) : The rate of absolute churners which are correctly predicted is known as\\nRecall, mathematically expressed as follows:\\nRC=Tp\\nTp+Fn(14)\\nPrecision (PC) : The rate of predicted churners which are precise is known as Precision,\\ncalculated as follows:\\nPC=Tp\\nTp+Fp(15)'), Document(metadata={'source': 'testDoc.pdf', 'page': 15}, page_content='Here, in ACC, RC, and PC, if the ‘True positive’ value is less then the ‘False positive’\\nvalue ( Tp<Fp), then accuracy will always increase when we have a prediction rule con-\\nsistently producing ‘negative’ output. Similarly, when ( Tn<Fn), the same will happen\\nwhen we have a rule that always gives a ‘positive’ output (Chicco and Jurman 2020 ). This\\nlimitation is called the accuracy paradox (Afonja 2017 ). To overcome this, we use another\\nperformance matrix, namely Matthew’s correlation coefficient (MCC).\\nMatthew’s Correlation Coefficient (MCC) : It is known as the ‘correlation coefficient’\\nbetween the predicted and actual value. It is mathematically defined as follows:\\nMCC =Tp×Tn−Fp×Fn√(Tp+Fp)×(Tp+Fn)×(T n+Fp)×(T n+Fn)(16)\\nHere, the MCC is used as a balanced measure because it includes all the four aforesaid cases\\nof the confusion matrix. It has a range of ‘-1’ to ‘1’ where ‘-1’ implies a wrong dichotomous'), Document(metadata={'source': 'testDoc.pdf', 'page': 15}, page_content='prediction, and ‘1’ implies correct dichotomous prediction. Thus, we can measure how well\\nthe prediction model performs.\\n5 Experimentalresultsanddiscussion\\nFor experimental purposes, we divide both the original and categorized dataset into training\\nand testing datasets. Each of these datasets is divided into the ratio of 80:20 for training\\nand testing purposes. We then obtain the prediction model by applying the training dataset294 JournalofIntelligentInformationSystems(2021)56:2 79–023'), Document(metadata={'source': 'testDoc.pdf', 'page': 16}, page_content='Table9 Results of the different machine learning algorithms for all employee class dataset and new categorized dataset\\nEmployee class SVM LR DT RF XGBoost CatBoost\\nACC 0.946 0.783 0.975 0.981 0.971 0.985\\nAll employee RC 0.951 0.781 0.981 0.984 0.972 0.989\\nclass PC 0.952 0.762 0.982 0.973 0.961 0.988\\nMCC 0.853 0.751 0.933 0.941 0.921 0.954\\nACC 0.944 0.887 0.942 0.985 0.969 0.989\\nEnthusiastic RC 0.951 0.892 0.983 0.991 0.971 0.996\\nclass PC 0.941 0.891 0.981 0.992 0.971 0.997\\nMCC 0.867 0.722 0.946 0.974 0.925 0.978\\nACC 0.960 0.945 0.985 0.991 0.975 0.994\\nBehavioral RC 0.961 0.940 0.989 0.990 0.981 0.991\\nclass PC 0.960 0.950 0.992 0.991 0.980 0.991\\nMCC 0.854 0.797 0.947 0.983 0.911 0.987\\nACC 0.953 0.922 0.981 0.983 0.984 0.989\\nDistressed RC 0.950 0.921 0.981 0.990 0.981 0.992\\nclass PC 0.941 0.931 0.982 0.982 0.972 0.991\\nMCC 0.903 0.840 0.962 0.975 0.967 0.981JournalofIntelligentInformationSystems(2021)56:2 79–023 295'), Document(metadata={'source': 'testDoc.pdf', 'page': 17}, page_content='and evaluate it using the testing dataset. To evaluate the experimental results, we adopt\\nthe Accuracy (ACC), Recall (RC), Precision (PC), and Matthew’s Correlation Coefficient\\n(MCC) as performance measures. The results for the prediction and retention phases in the\\ncase of both the original dataset as well as the new categorized dataset are presented in the\\nsubsequent sections. The results achieved by CatBoost are shown to have better prediction\\nperformance than other existing ML algorithms.\\n5.1 Experimentalresultsforpredictionphase\\nTable 9shows the experimental results of various individual and tree-based ensemble\\nmachine learning algorithms, which we have considered for experimental study. The\\nlearning performance are shown in terms of ACC, RC, PC, and MCC performance metrics.\\nThe results marked in boldface for each class of employees conclude that the CatBoost\\nalgorithm provides the best prediction performance having the highest MCC. Fig. 5a, b, c,'), Document(metadata={'source': 'testDoc.pdf', 'page': 17}, page_content='and d display the prediction performance for the dataset of ‘All employee class’ (without\\nusing the AEIM model), ‘Enthusiastic employee class’, ‘Behavioral employee class’ and\\n‘Distressed employee class’, respectively. From the results, we conclude that the prediction\\nperformance with the categorized dataset (using AEIM model) is better in terms of ACC\\nand MCC as compared to those achieved without using the AEIM model.\\nIt can be further concluded from Table 10that prediction results of the CatBoost algo-\\nrithm for categorized datasets are far better as compared to the results obtained using\\nthe original dataset. In this table, E CatBoost, B CatBoost, and D CatBoost show the\\nMCC for enthusiastic, behavioral, and distressed employee class, respectively. Whereas\\nthe Avg CatBoost shows the average MCC of all the categorized employee classes, and\\nAllCatBoost shows the MCC for the original employee dataset (All employee class).\\n0.70.750.80.850.90.951\\nSVM LR DT RF XGboost Catboost'), Document(metadata={'source': 'testDoc.pdf', 'page': 17}, page_content='ACC MCC0.70.750.80.850.90.951\\nSVM LR DT RF XGboost CatBoost\\nACC MCC\\n0.70.750.80.850.90.951\\nSVM LR DT RF XGboost CatBoost\\nACC MCC0.70.750.80.850.90.951\\nSVM LR DT RF XGboost CatBoost\\nACC MCCa b\\nc d\\nFig.5 Prediction performance of the different machine learning algorithms for a‘All employee class’ (with-\\nout AEIM) b‘Enthusiastic employee class’, c‘Behavioral employee class’, and d‘Distressed employee\\nclass’296 JournalofIntelligentInformationSystems(2021)56:2 79–023'), Document(metadata={'source': 'testDoc.pdf', 'page': 18}, page_content='Table10 Comparative prediction performance result with 2.80% average improvement in MCC with proposed ECPR scheme\\nECatBoost B CatBoost D CatBoost Avg CatBoost All CatBoost\\nMCC 0.978 0.0.987 0.981 0.982 0.954JournalofIntelligentInformationSystems(2021)56:2 79–023 297'), Document(metadata={'source': 'testDoc.pdf', 'page': 19}, page_content=\"0.9780.987\\n0.981 0.982\\n0.954MCC\\nFig.6 Bar graph showing 2.80% average improvement in term of MCC using proposed ECPR scheme\\nThus, we can summarize from Fig. 6that using the categorized dataset achieves higher\\nprediction accuracy over the original dataset for ECn problem. The proposed ECPR scheme\\nwith the AEIM model can perform better than directly using traditional machine learning\\nalgorithms.\\n19%\\n29%\\n19%18%15%Importance of feature for 'All employee class'.\\nnumber_project\\nsatisfaction_level\\naverage_montly_hours\\nlast_evaluation\\ntime_spend_company9%\\n47%\\n13%6%25%Importance of feature for 'E nthusiastic employee class'.\\nnumber_project\\nsatisfaction_level\\naverage_montly_hours\\nlast_evaluation\\ntime_spend_company\\n25%\\n23%21%21%10%Importance of feature for 'Behavioral employee class'.\\nnumber_project\\nsatisfaction_level\\naverage_montly_hours\\nlast_evaluation\\ntime_spend_company30%\\n26%24%15%5%Importance of feature for 'Distressed employee class'. \\nnumber_project\\nsatisfaction_level\\naverage_montly_hours\"), Document(metadata={'source': 'testDoc.pdf', 'page': 19}, page_content='last_evaluation\\ntime_spend_companya\\ncb\\nd\\nFig. 7 Class wise primary reasons for employee quitting a‘All employee class’ (without using AEIM), b\\n‘Enthusiastic employee class’, c‘Behavioral employee class’, d‘Distressed employee class’298 JournalofIntelligentInformationSystems(2021)56:2 79–023'), Document(metadata={'source': 'testDoc.pdf', 'page': 20}, page_content='5.2 Experimentalresultsforretentionpolicymanagement\\nAs discussed earlier, the process of finding the primary reasons for employee quitting\\nconsists of two steps. First, we rank features according to their importance using the\\npermutation-based feature importance method. Next, we make a subset of top-k features.\\nThen, the retention policy is designed based on the outcome of the prediction phase and the\\nsubset of the top-k features.\\nFigure 7a shows the top-5 vital reasons for the original dataset, which may be the prime\\ncause of employees quitting the organization. The results show that ‘satisfaction level’ has\\nthe highest (29%) percentage of correlation with the target feature (‘left’). Next, Fig. 7b, c,\\nand d show the top-5 vital reasons for enthusiastic, behavioral, and distressed employee\\nclass, respectively. It is observed from the figures that the top-5 correlated features and\\ntheir respective importance values are different for each employee class. In the case of'), Document(metadata={'source': 'testDoc.pdf', 'page': 20}, page_content='enthusiastic employees, ‘satisfaction level’ feature is the most important cause of quitting,\\nwhile in the case of behavioral and distressed employees, the ‘number project’ feature has\\nthe highest correlation value with target feature (‘left’).\\nThus, it can be concluded that by considering the important features of each respective\\nemployee class may help the organizations (especially for the large organizations) to make a\\nmore productive, cost-effective, and timely retention policy to stop employees from quitting\\ntheir job.\\n6 Conclusion\\nIn this paper, we have designed a MADM based scheme for ECn problem, referred to as\\nemployee churn prediction and retention (ECPR) scheme. The ECPR primarily sheds light\\non three key aspects. First, a novel accomplishment-based employee importance model\\n(AEIM) has been proposed to categorize the employees into various classes based on their\\nimportance value by utilizing a multi-attribute decision making (MADM) approach. Sec-'), Document(metadata={'source': 'testDoc.pdf', 'page': 20}, page_content='ond, the CatBoost ensemble machine learning algorithm has been applied to judge its\\npotential for class-wise prediction of churning employees’ attrition. Finally, the attributes\\nresponsible for employee attrition have been identified using permutation-based feature\\nimportance method so as to retain valuable employees. The benchmark dataset for the\\nhuman resources information system (HRIS) has been used as an input to the ECPR scheme.\\nThe performance of the proposed scheme has been evaluated in terms of accuracy (ACC),\\nRecall (RC), Precision (PC) and Matthew’s Correlation Coefficient (MCC). The compar-\\native results demonstrate that ECPR based on CatBoost algorithm has higher prediction\\naccuracy rates, outperforming the direct prediction method by at least 2.80%. Note that our\\nsystem is an effective tool for predicting the risk of churn in any organization, particularly\\nin those with higher churn rates, such as telecommunications, where even small improve-'), Document(metadata={'source': 'testDoc.pdf', 'page': 20}, page_content='ments in the accuracy of churn prediction models can be linked to significant financial\\ngains. In addition, the results for the retention phase depicts different responsible attributes\\nfor attrition under different employee categories.\\nThe results of the proposed ECPR scheme on the HRIS dataset shows its superiority over\\nother traditional models and it is expected that the same model may produce reasonable\\nresults on other datasets having similar feature sets. However, it would be wise to test this\\nsystem with other real time data from some large organizations. In the future, we plan to\\napply the proposed scheme to different organizations, which could help to generate real-time\\nsolutions with more promising performance.JournalofIntelligentInformationSystems(2021)56:2 79–023 299'), Document(metadata={'source': 'testDoc.pdf', 'page': 21}, page_content='CompliancewithEthicalStandards\\nC o n f l i c to fi n t e r e s t s The authors declare that they have no conflict of interest.\\nReferences\\nAfonja, T. (2017). Accuracy paradox, towards data science. https://towardsdatascience.com/ accuracy-paradox-\\n897a69e2dd9b , Online: Stand 27. July 2020.\\nBreiman, L. (2001). Random forests. Machine Learning ,45(1), 5–32.\\nBrown, D., & Wilson, S. (2007). The black books of outsourcing: How to manage the changes, challenges,\\nand opportunities. Wiley.\\nBudhwar, P.S., Varma, A., Singh, V ., Dhar, R. (2006). HRM systems of indian call centres: an exploratory\\nstudy. The International Journal of Human Resource Management ,17(5), 881–897. https://doi.org/10.\\n1080/09585190600640976 .\\nChen, T., He, T., Benesty, M., Khotilovich, V ., Tang, Y . (2015). Xgboost: extreme gradient boosting. R\\npackage version 04-2 pp 1–4.\\nChicco, D., & Jurman, G. (2020). The advantages of the matthews correlation coefficient (mcc) over f1 score'), Document(metadata={'source': 'testDoc.pdf', 'page': 21}, page_content='and accuracy in binary classification evaluation. BMC Genomics ,21(1), 6.\\nChien, C.F., & Chen, L.F. (2008). Data mining to improve personnel selection and enhance human cap-\\nital: a case study in high-technology industry. Expert Systems with Applications ,34(1), 280–290.\\nhttps://doi.org/10.1016/j.eswa.2006.09.003 .\\nChu, A.T.W., Kalaba, R.E., Spingarn, K. (1979). A comparison of two methods for determining the\\nweights of belonging to fuzzy sets. Journal of Optimization Theory and Applications ,27(4), 531–538.\\nhttps://doi.org/10.1007/bf00933438 .\\nCortes, C., & Vapnik, V . (1995). Support vector machine. Machine Learning ,20(3), 273–297.\\nDolatabadi, S.H., & Keynia, F. (2017). Designing of customer and employee churn prediction model based\\non data mining method and neural predictor. In 2017 2nd International Conference on Computer and\\nCommunication Systems (ICCCS) : IEEE. https://doi.org/10.1109/ccoms.2017.8075270 .'), Document(metadata={'source': 'testDoc.pdf', 'page': 21}, page_content='Fan, C.Y ., Fan, P.S., Chan, T.Y ., Chang, S.H. (2012). Using hybrid data mining and machine learning cluster-\\ning analysis to predict the turnover rate for technology professionals. Expert Systems with Applications ,\\n39(10), 8844–8851. https://doi.org/10.1016/j.eswa.2012.02.005 .\\nFazlollahtabar, H. (2010). A subjective framework for seat comfort based on a heuristic multi criteria deci-\\nsion making technique and anthropometry. Applied Ergonomics ,42(1), 16–28. https://doi.org/10.1016/j.\\napergo.2010.04.004 .\\nFisher, A., Rudin, C., Dominici, F. (2018). All models are wrong but many are useful: Variable importance for\\nblack-box, proprietary, or misspecified prediction models, using model class reliance. arXiv: 180101489 .\\nFrederiksen, A. (2017). Job satisfaction and employee turnover: a firm-level perspective. German Journal of\\nHuman Resource Management ,31(2), 132–161.\\nFrierson, J., & Si, D. (2018). Who’s next: Evaluating attrition with machine learning algorithms and'), Document(metadata={'source': 'testDoc.pdf', 'page': 21}, page_content='survival analysis. In Big data – BigData 2018 (pp. 251–259): Springer International Publishing.\\nhttps://doi.org/10.1007/978-3-319-94301-5 19.\\nGao, X., Wen, J., Zhang, C. (2019). An improved random forest algorithm for predicting employee turnover.\\nMathematical Problems in Engineering ,2019 , 1–12. https://doi.org/10.1155/2019/4140707 .\\nGhasemaghaei, M., & Calic, G. (2019). Can big data improve firm decision quality? the role of data quality\\nand data diagnosticity. Decision Support Systems ,120, 38–49. https://doi.org/10.1016/j.dss.2019.03.008 .\\nHarter, J.K., Schmidt, F.L., Hayes, T.L. (2002). Business-unit-level relationship between employee satisfac-\\ntion, employee engagement, and business outcomes: a meta-analysis. Journal of Applied Psychology ,\\n87(2), 268.\\nHom, P.W., Lee, T.W., Shaw, J.D., Hausknecht, J.P. (2017). One hundred years of employee turnover theory\\nand research. Journal of Applied Psychology ,102(3), 530–545. https://doi.org/10.1037/apl0000103 .'), Document(metadata={'source': 'testDoc.pdf', 'page': 21}, page_content='Hwang, C.L., & Yoon, K. (1981). Multiple attribute decision making . Berlin: Springer. https://doi.org/10.\\n1007/978-3-642-48318-9 .\\nJin, C., De-lin, L., Fen-xiang, M. (2009). An improved ID3 decision tree algorithm. In 2009 4th International\\nConference on Computer Science & Education : IEEE. https://doi.org/10.1109/iccse.2009.5228509 .\\nKaggle (2017). Hr analytics dataset. https://www.kaggle.com/lnvardanyan/hr-analytics#turnover.csv , Online;\\nStand 04. April 2020.300 JournalofIntelligentInformationSystems(2021)56:2 79–023'), Document(metadata={'source': 'testDoc.pdf', 'page': 22}, page_content='Khodabakhsh, M., Kahani, M., Bagheri, E. (2018). Predicting future personal life events on twitter via\\nrecurrent neural networks. Journal of Intelligent Information Systems. https://doi.org/10.1007/s10844-\\n018-0519-2 .\\nKoh, H.C., & Goh, C.T. (1995). An analysis of the factors affecting the turnover intention of non-managerial\\nclerical staff: a Singapore study. The International Journal of Human Resource Management ,6(1), 103–\\n125. https://doi.org/10.1080/09585199500000005 .\\nKohavi, R., & Li, C.H. (1995). Oblivious decision trees graphs and top down pruning. In Proceedings of\\nthe 14th International joint conference on artificial intelligence , (V ol. 2 pp. 1071–1077). San Francisco:\\nMorgan Kaufmann Publishers Inc. IJCAI’95.\\nKrylovas, A., Dadelo, S., Kosareva, N., Zavadskas, E.K. (2017). Entropy–KEMIRA approach for MCDM\\nproblem solution in human resources selection task. International Journal of Information Technology &,'), Document(metadata={'source': 'testDoc.pdf', 'page': 22}, page_content='Decision Making ,16(05), 1183–1209. https://doi.org/10.1142/s0219622017500274 .\\nLu, L., & Yuan, Y . (2018). A novel TOPSIS evaluation scheme for cloud service trustworthiness combining\\nobjective and subjective aspects. Journal of Systems and Software ,143, 71–86. https://doi.org/10.1016/j.\\njss.2018.05.004 .\\nMendoza-G ˙omez, R., R ´ıos-Mercado, V ., Valenzuela-Oca ˜na, K.B. (2019). An efficient decision-making\\napproach for the planning of diagnostic services in a segmented healthcare system. International Journal\\nof Information Technology &, Decision Making , 1–35, https://doi.org/10.1142/s0219622019500196 .\\nMicci-Barreca, D. (2001). A preprocessing scheme for high-cardinality categorical attributes in classification\\nand prediction problems. SIGKDD Explor Newsl ,3(1), 27–32. https://doi.org/10.1145/507533.507538 .\\nMorrow, P.C., McElroy, J.C., Laczniak, K.S., Fenton, J.B. (1999). Using absenteeism and performance to'), Document(metadata={'source': 'testDoc.pdf', 'page': 22}, page_content='predict employee turnover: Early detection through company records. Journal of Vocational Behavior ,\\n55(3), 358–374. https://doi.org/10.1006/jvbe.1999.1687 .\\nPargent, F., Bischl, B., Thomas, J. (2019). A benchmark experiment on how to encode categorical features in\\npredictive modeling. Master Thesis in Statistics, Ludwig-Maximilians-Universit ¨at M¨unchen, Leopoldstr\\n13, 80802 M¨ unchen.\\nProkhorenkova, L., Gusev, G., V orobev, A., Dorogush, A.V ., Gulin, A. (2018). Catboost: unbiased boosting\\nwith categorical features. In Advances in neural information processing systems (pp. 6638–6648).\\nPunnoose, R., & Ajit, P. (2016). Prediction of employee turnover in organizations using machine\\nlearning algorithms. International Journal of Advanced Research in Artificial Intelligence\\n5(9)https://doi.org/10.14569/ijarai.2016.050904 .\\nRashid, T.A., & Jabar, A.L. (2016). Improvement on predicting employee behaviour through intelligent'), Document(metadata={'source': 'testDoc.pdf', 'page': 22}, page_content='techniques. IET Networks ,5(5), 136–142. https://doi.org/10.1049/iet-net.2015.0106 .\\nRen, C., & Li, H. (2011). Analysis on human resource management of travel agencies. In 2011 International\\nConference on Computer Science and Service System (CSSS) : IEEE. https://doi.org/10.1109/csss.2011.\\n5974582 .\\nSaaty, R. (1987). The analytic hierarchy process—what it is and how it is used. Mathematical Modelling ,\\n9(3-5), 161–176. https://doi.org/10.1016/0270-0255(87)90473-8 .\\nSanders, R. (1987). The pareto principle: its use and abuse. Journal of Services Marketing ,1(2), 37–40.\\nhttps://doi.org/10.1108/eb024706 .\\nSaradhi, V .V ., & Palshikar, G.K. (2011). Employee churn prediction. Expert Systems with Applications ,38(3),\\n1999–2006. https://doi.org/10.1016/j.eswa.2010.07.134 .\\nSexton, R.S., McMurtrey, S., Michalopoulos, J.O., Smith, A.M. (2005). Employee turnover: a neural network\\nsolution. Computers & Operations Research ,32(10), 2635–2651. https://doi.org/10.1016/j.cor.2004.\\n06.022 .'), Document(metadata={'source': 'testDoc.pdf', 'page': 22}, page_content='Shannon, C.E. (2001). A mathematical theory of communication. ACM SIGMOBILE Mobile Computing and\\nCommunications Review ,5(1), 3–55.\\nSikaroudi, E., Mohammad, A., Ghousi, R., Sikaroudi, A. (2015). A data mining approach to employee\\nturnover prediction (case study: Arak automotive parts manufacturing). Journal of Industrial and\\nSystems Engineering ,8(4), 106–121.\\nTarnowska, K., & Ras, Z. (2018). From knowledge discovery to customer attrition. In Lecture Notes in\\nComputer Science (pp. 417–425): Springer International Publishing. https://doi.org/10.1007/978-3-030-\\n01851-1 40.\\nTarnowska, K., Ras, Z.W., Daniel, L. (2020). Recommender system for improving customer loyalty. Springer\\nInternational Publishing, https://doi.org/10.1007/978-3-030-13438-9 .\\nTas¸abat, S.E. (2019). A novel multicriteria decision-making method based on distance, similarity, and cor-\\nrelation: DSC TOPSIS. Mathematical Problems in Engineering ,2019 , 1–20. https://doi.org/10.1155/\\n2019/9125754 .'), Document(metadata={'source': 'testDoc.pdf', 'page': 22}, page_content='Tomar, A., & Jana, P.K. (2018). Mobile charging of wireless sensor networks for internet of things: A multi-\\nattribute decision making approach. In Distributed Computing and Internet Technology (pp. 309–324 ):\\nSpringer International Publishing. https://doi.org/10.1007/978-3-030-05366-6 26.JournalofIntelligentInformationSystems(2021)56:2 79–023 301'), Document(metadata={'source': 'testDoc.pdf', 'page': 23}, page_content='Wang, T.C., & Lee, H.D. (2009). Developing a fuzzy topsis approach based on subjective weights and\\nobjective weights. Expert Systems with Applications ,36(5), 8980–8985.\\nWebb, G.I., Sammut, C., Perlich, C., Horv ´ath, T., Wrobel, S., Korb, K.B., Noble, W.S., Leslie,\\nC., Lagoudakis, M.G., Quadrianto, N., Buntine, W.L., Quadrianto, N., Buntine, W.L., Getoor,\\nL., Namata, G., Getoor, L., Xin Jin, J.H., Ting, J.A., Vijayakumar, S., Schaal, S., Raedt, L.D.\\n(2011). Logistic regression. In Encyclopedia of Machine Learning (pp. 631–631). US: Springer.\\nhttps://doi.org/10.1007/978-0-387-30164-8 493.\\nXiao, J., Zhu, X., Huang, C., Yang, X., Wen, F., Zhong, M. (2019). A new approach for stock price analysis\\nand prediction based on SSA and SVM. International Journal of Information Technology & Decision\\nMaking ,18(01), 287–310. https://doi.org/10.1142/s021962201841002x .\\nYeh, C.H. (2002). A problem-based selection of multi-attribute decision-making methods. International'), Document(metadata={'source': 'testDoc.pdf', 'page': 23}, page_content='Transactions in Operational Research ,9(2), 169–181.\\nYigit, I.O., & Shourabizadeh, H. (2017). An approach for predicting employee churn by using data min-\\ning. In 2017 International Artificial Intelligence and Data Processing Symposium (IDAP) : IEEE.\\nhttps://doi.org/10.1109/idap.2017.8090324 .\\nZhou, M., Liu, X.B., Chen, Y .W., Yang, J.B. (2018). Evidential reasoning rule for madm with both weights\\nand reliabilities in group decision making. Knowledge-Based Systems ,143, 142–161.\\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps\\nand institutional affiliations.302 JournalofIntelligentInformationSystems(2021)56:2 79–023')]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T07:52:34.990813Z",
     "start_time": "2024-07-15T07:52:34.985212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "API_KEY = \"<KEY>\"\n",
    "def get_Gemini_Response(prompt):\n",
    "    try:\n",
    "        genai.configure(api_key=API_KEY)\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ],
   "id": "526015761ff9d815",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "161f896170a95659"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
